import os
import tempfile
import streamlit as st
from dotenv import load_dotenv
from openai import AzureOpenAI
import azure.cognitiveservices.speech as speechsdk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import requests
import rarfile

st.set_page_config(page_title="ÙØµÙŠØ­ - Ù…Ù†ØµØ© ØªØµØ­ÙŠØ­ ÙˆØªØ­Ù„ÙŠÙ„", layout="centered")

st.markdown("""
    <style>
        .block-container {padding-top: 2rem; padding-bottom: 2rem;}
        .stButton>button {
            background-color: #005f73;
            color: white;
            padding: 0.6rem 1.5rem;
            font-size: 1.1rem;
            border-radius: 8px;
            font-weight: bold;
        }
        .stButton>button:hover {
            background-color: #0a9396;
        }
    </style>
""", unsafe_allow_html=True)

st.image("logo_faseeh.png", width=120)
st.markdown("<h1 style='text-align:center;'>ÙØµÙŠØ­</h1>", unsafe_allow_html=True)
st.markdown("<h4 style='text-align:center; color: gray;'>Ù…Ù†ØµØ© Ø°ÙƒÙŠØ© Ù„ØªØµØ­ÙŠØ­ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</h4>", unsafe_allow_html=True)

load_dotenv()

client = AzureOpenAI(
    api_key=os.getenv("AZURE_API_KEY"),
    api_version=os.getenv("AZURE_API_VERSION"),
    azure_endpoint=os.getenv("AZURE_ENDPOINT")
)
deployment_name = os.getenv("DEPLOYMENT_NAME")
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")


@st.cache_resource
def load_sentiment_model():
    model_path = "./best_model_dir"
    if not os.path.exists(model_path):
        with st.spinner("ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Google Drive..."):
            gdrive_url = "https://drive.google.com/uc?id=1-SBx3ufWij2kUm8SGfOLKCU7zCMGEd9t"
            rar_file = os.path.join(tempfile.gettempdir(), "model.rar")

            
            response = requests.get(gdrive_url, allow_redirects=True)
            with open(rar_file, "wb") as f:
                f.write(response.content)

            
            with rarfile.RarFile(rar_file) as rf:
                rf.extractall(model_path)

    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer

sentiment_model, sentiment_tokenizer = load_sentiment_model()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
sentiment_model.to(device)

label_map = {0: 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 1: 'Ø³Ù„Ø¨ÙŠ', 2: 'Ù…Ø­Ø§ÙŠØ¯'}
color_map = {'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ': '#90ee90', 'Ø³Ù„Ø¨ÙŠ': '#ff7f7f', 'Ù…Ø­Ø§ÙŠØ¯': '#d3d3d3'}

voice_options = {
    "ğŸ‡ªğŸ‡¬ Ù…ØµØ±ÙŠ (Salma)": "ar-EG-SalmaNeural",
    "ğŸ‡¸ğŸ‡¦ Ø³Ø¹ÙˆØ¯ÙŠ (Hamed)": "ar-SA-HamedNeural",
    "ğŸ‡¸ğŸ‡¦ Ø³Ø¹ÙˆØ¯ÙŠ (Zariyah)": "ar-SA-ZariyahNeural",
    "ğŸ‡¦ğŸ‡ª Ø¥Ù…Ø§Ø±Ø§ØªÙŠ (Fatima)": "ar-AE-FatimaNeural",
    "ğŸ‡±ğŸ‡§ Ù„Ø¨Ù†Ø§Ù†ÙŠ (Layla)": "ar-LB-LaylaNeural",
    "ğŸ‡¸ğŸ‡¾ Ø³ÙˆØ±ÙŠ (Amany)": "ar-SY-AmanyNeural",
    "ğŸ‡©ğŸ‡¿ Ø¬Ø²Ø§Ø¦Ø±ÙŠ (Amina)": "ar-DZ-AminaNeural",
    "ğŸ‡²ğŸ‡¦ Ù…ØºØ±Ø¨ÙŠ (Jamal)": "ar-MA-JamalNeural",
    "ğŸ‡¹ğŸ‡³ ØªÙˆÙ†Ø³ÙŠ (Hedi)": "ar-TN-HediNeural"
}

def correct_arabic_text(user_text):
    prompt = f"""ØªØµØ±Ù ÙÙ‚Ø· ÙƒÙ…Ø¯Ù‚Ù‚ Ø¥Ù…Ù„Ø§Ø¦ÙŠ. Ù„Ø§ ØªØ¹ÙŠØ¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø¬Ù…Ù„Ø©ØŒ ÙˆÙ„Ø§ ØªØºÙŠÙ‘Ø± ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ ÙˆÙ„Ø§ ØªØµØ­Ø­ Ø§Ù„Ù†Ø­Ùˆ Ø£Ùˆ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨. 
Ù…Ù‡Ù…ØªÙƒ Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù‡ÙŠ ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ø§Ù„Ø¸Ø§Ù‡Ø±Ø© ÙÙ‚Ø·ØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¬Ù…Ù„Ø© Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ø§Ù…ÙŠØ©. 
Ø£ÙŠ ÙƒÙ„Ù…Ø© ØµØ­ÙŠØ­Ø© Ù„Ø§ ØªØºÙŠÙ‘Ø±Ù‡Ø§ØŒ ÙˆÙ„Ø§ ØªØ­ÙˆÙ‘Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø¥Ù„Ù‰ ÙØµØ­Ù‰.
ÙŠØ±Ø¬Ù‰ ØªØ¬Ø§Ù‡Ù„ Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…ÙŠØ© Ø°Ø§Øª Ø¯Ù„Ø§Ù„Ø§Øª Ø¹Ø§Ø·ÙÙŠØ© ÙˆÙ„Ø§ ØªØ¹ØªØ¨Ø±Ù‡Ø§ Ø­Ø³Ø§Ø³Ø©ØŒ ÙˆØ§ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ØºÙŠØ± Ø¶Ø§Ø±.

Ø§Ù„Ø¬Ù…Ù„Ø©:

{user_text}

Ø£Ø¹Ø·Ù†ÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØµØ­Ø­Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø¥Ø¶Ø§ÙØ§Øª Ø£Ùˆ Ø´Ø±Ø­."""
    try:
        response = client.chat.completions.create(
            model=deployment_name,
            messages=[
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø¯Ù‚Ù‚ Ù„ØºÙˆÙŠ Ø°ÙƒÙŠ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=100
        )
        message = response.choices[0].message.content if response.choices[0].message else None
        return message.strip() if message else " Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø¯ Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."
    except Exception as e:
        return f" Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ OpenAI: {str(e)}"

def analyze_sentiment(text):
    inputs = sentiment_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = sentiment_model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        pred_label = torch.argmax(probs).item()
    return label_map[pred_label], probs[0][pred_label].item()

def azure_text_to_speech(text, key, region, voice):
    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)
    speech_config.speech_synthesis_voice_name = voice
    synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)
    result = synthesizer.speak_text_async(text).get()
    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
        temp_audio_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        with open(temp_audio_file.name, "wb") as f:
            f.write(result.audio_data)
        return temp_audio_file.name
    return None

user_input = st.text_area("ğŸ“ Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµØ­ÙŠØ­Ù‡Ø§:", height=150)
selected_voice_label = st.selectbox("ğŸ™ï¸ Ø§Ø®ØªØ± Ø§Ù„Ù„Ù‡Ø¬Ø©/Ø§Ù„ØµÙˆØª:", list(voice_options.keys()))
selected_voice = voice_options[selected_voice_label]

if st.button("âœ¨ ØªÙ†ÙÙŠØ°"):
    if user_input.strip():
        with st.spinner("ğŸ”§ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØµØ­ÙŠØ­..."):
            corrected = correct_arabic_text(user_input)

        st.markdown("### Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¬Ù…Ù„:")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ:**")
            st.write(user_input)
        with col2:
            st.markdown("**Ø¨Ø¹Ø¯ Ø§Ù„ØªØµØ­ÙŠØ­:**")
            st.markdown(f"""
                <div style="background-color:#e3f2fd;padding:10px;border-radius:8px;">
                    {corrected}
                </div>
            """, unsafe_allow_html=True)

        with st.spinner("ğŸ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±..."):
            sentiment_label, confidence = analyze_sentiment(corrected)
        color = color_map[sentiment_label]
        st.markdown(
            f"<h4 style='color:{color}'>ğŸ” Ø§Ù„Ù†ØªÙŠØ¬Ø©: {sentiment_label}</h4>", unsafe_allow_html=True
        )

        with st.spinner("ğŸ”Š ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª..."):
            audio_path = azure_text_to_speech(corrected, SPEECH_KEY, SPEECH_REGION, selected_voice)
        if audio_path:
            st.audio(audio_path, format="audio/mp3")
        else:
            st.error(" Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØª.")

        st.markdown("""
            <hr style='margin-top: 2rem;'>
            <div style='text-align: center; font-size: 0.85rem; color: #888'>
                Â© 2025 ÙØµÙŠØ­ - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø©
            </div>
        """, unsafe_allow_html=True)
    else:
        st.warning("Ù…Ù† ÙØ¶Ù„Ùƒ Ø£Ø¯Ø®Ù„ Ø¬Ù…Ù„Ø© Ø£ÙˆÙ„Ø§Ù‹.")
